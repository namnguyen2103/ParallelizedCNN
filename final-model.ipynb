{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ce206c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-23T15:32:36.591733Z",
     "iopub.status.busy": "2024-06-23T15:32:36.591425Z",
     "iopub.status.idle": "2024-06-23T15:32:36.612177Z",
     "shell.execute_reply": "2024-06-23T15:32:36.611291Z"
    },
    "papermill": {
     "duration": 0.028778,
     "end_time": "2024-06-23T15:32:36.614113",
     "exception": false,
     "start_time": "2024-06-23T15:32:36.585335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_cpu.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_cpu.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "#include <float.h>\n",
    "#include <time.h>\n",
    "#include <string.h>\n",
    "\n",
    "#define NUM_TRAIN_IMAGES 200\n",
    "#define NUM_TEST_IMAGES 50\n",
    "#define IMG_HEIGHT 64\n",
    "#define IMG_WIDTH 64\n",
    "#define IMG_DEPTH 3\n",
    "#define FLATTENED_SIZE (IMG_HEIGHT * IMG_WIDTH * IMG_DEPTH)\n",
    "#define NUM_CLASSES 1\n",
    "#define BATCH_SIZE 100\n",
    "\n",
    "#define EPSILON 1e-10\n",
    "\n",
    "// Define activation functions\n",
    "double sigmoid(double x) {\n",
    "    return 1.0 / (1.0 + exp(-x));\n",
    "}\n",
    "\n",
    "double stable_sigmoid(double x) {\n",
    "    double sig = sigmoid(x);\n",
    "    return fmax(fmin(sig, 1.0 - EPSILON), EPSILON);\n",
    "}\n",
    "\n",
    "double sigmoid_derivative(double x) {\n",
    "    double sx = sigmoid(x);\n",
    "    return sx * (1 - sx);\n",
    "}\n",
    "\n",
    "double relu(double x) {\n",
    "    return x > 0 ? x : 0.0;\n",
    "}\n",
    "\n",
    "double relu_derivative(double x) {\n",
    "    return x > 0 ? 1.0 : 0.0;\n",
    "}\n",
    "\n",
    "double random_normal() {\n",
    "    double u1 = (double)rand() / RAND_MAX;\n",
    "    double u2 = (double)rand() / RAND_MAX;\n",
    "    return sqrt(-2.0 * log(u1)) * cos(2.0 * M_PI * u2);\n",
    "}\n",
    "\n",
    "void he_init(double *weights, int fan_in, int fan_out) {\n",
    "    double stddev = sqrt(2.0 / fan_in);\n",
    "    for (int i = 0; i < fan_in * fan_out; i++) {\n",
    "        weights[i] = random_normal() * stddev;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Define the Neural Network structure\n",
    "typedef struct {\n",
    "    int input_size;\n",
    "    int hidden_size;\n",
    "    int output_size;\n",
    "    double *w1;\n",
    "    double *b1;\n",
    "    double *w2;\n",
    "    double *b2;\n",
    "    double *z1;\n",
    "    double *a1;\n",
    "    double *z2;\n",
    "    double *a2;\n",
    "} NeuralNetwork;\n",
    "\n",
    "// Initialize the neural network\n",
    "void init_nn(NeuralNetwork *nn, int input_size, int hidden_size, int output_size) {\n",
    "    nn->input_size = input_size;\n",
    "    nn->hidden_size = hidden_size;\n",
    "    nn->output_size = output_size;\n",
    "\n",
    "    nn->w1 = (double *)malloc(input_size * hidden_size * sizeof(double));\n",
    "    nn->b1 = (double *)malloc(hidden_size * sizeof(double));\n",
    "    nn->w2 = (double *)malloc(hidden_size * output_size * sizeof(double));\n",
    "    nn->b2 = (double *)malloc(output_size * sizeof(double));\n",
    "\n",
    "    nn->z1 = (double *)malloc(hidden_size * sizeof(double));\n",
    "    nn->a1 = (double *)malloc(hidden_size * sizeof(double));\n",
    "    nn->z2 = (double *)malloc(output_size * sizeof(double));\n",
    "    nn->a2 = (double *)malloc(output_size * sizeof(double));\n",
    "\n",
    "    if (!nn->w1 || !nn->b1 || !nn->w2 || !nn->b2 || !nn->z1 || !nn->a1 || !nn->z2 || !nn->a2) {\n",
    "        perror(\"Error allocating memory\");\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "\n",
    "    he_init(nn->w1, input_size, hidden_size);\n",
    "    he_init(nn->w2, hidden_size, output_size);\n",
    "}\n",
    "\n",
    "// Forward propagation\n",
    "void forward(NeuralNetwork *nn, double *input) {\n",
    "    for (int i = 0; i < nn->hidden_size; i++) {\n",
    "        nn->z1[i] = 0.0;\n",
    "        for (int j = 0; j < nn->input_size; j++) {\n",
    "            nn->z1[i] += input[j] * nn->w1[j * nn->hidden_size + i];\n",
    "        }\n",
    "        nn->z1[i] += nn->b1[i];\n",
    "        nn->a1[i] = relu(nn->z1[i]);\n",
    "    }\n",
    "\n",
    "    for (int i = 0; i < nn->output_size; i++) {\n",
    "        nn->z2[i] = 0.0;\n",
    "        for (int j = 0; j < nn->hidden_size; j++) {\n",
    "            nn->z2[i] += nn->a1[j] * nn->w2[j * nn->output_size + i];\n",
    "        }\n",
    "        nn->z2[i] += nn->b2[i];\n",
    "        nn->a2[i] = stable_sigmoid(nn->z2[i]);\n",
    "    }\n",
    "    //printf(\"Activations a2 (predictions):\\n\");\n",
    "    //for (int i = 0; i < nn->output_size; i++) {\n",
    "        //printf(\"%f \", nn->a2[i]);\n",
    "    //}\n",
    "    //printf(\"\\n\");\n",
    "}\n",
    "\n",
    "// Compute loss\n",
    "double compute_loss(double *y_true, double *y_pred, int size) {\n",
    "\n",
    "    double loss = 0.0;\n",
    "    for (int i = 0; i < size; i++) {\n",
    "        double pred = y_pred[i];\n",
    "        // Avoid log(0) and log(1)\n",
    "        if (pred<1e-8)\n",
    "            pred=1e-8;\n",
    "        if(pred>1-1e-8)\n",
    "            pred=1-1e-8;\n",
    "\n",
    "        double true_value = y_true[i];\n",
    "        double term1 = true_value * log(pred);\n",
    "        double term2 = (1 - true_value) * log(1 - pred);\n",
    "\n",
    "        //if (isnan(pred) || isnan(term1) || isnan(term2)) {\n",
    "            //printf(\"NaN detected at index %d: true_value=%f, pred=%f, term1=%f, term2=%f\\n\", i, true_value, pred, term1, term2);\n",
    "            //return -1; // Return an error code or handle it appropriately\n",
    "        //}\n",
    "\n",
    "        loss += -(term1 + term2);\n",
    "    }\n",
    "    return loss / size;\n",
    "}\n",
    "\n",
    "// Compute gradients\n",
    "void backward(NeuralNetwork *nn, double *input, double *y_true, double *dw1, double *db1, double *dw2, double *db2) {\n",
    "    double dL_da2[nn->output_size];\n",
    "    double da2_dz2[nn->output_size];\n",
    "    double dL_da1[nn->hidden_size];\n",
    "    double da1_dz1[nn->hidden_size];\n",
    "\n",
    "    for (int i = 0; i < nn->output_size; i++) {\n",
    "        dL_da2[i] = -(*(y_true + i) / nn->a2[i] - (1 - *(y_true + i)) / (1 - nn->a2[i]));\n",
    "        da2_dz2[i] = sigmoid_derivative(nn->z2[i]);\n",
    "        db2[i] = dL_da2[i] * da2_dz2[i];\n",
    "        for (int j = 0; j < nn->hidden_size; j++) {\n",
    "            dw2[j * nn->output_size + i] = nn->a1[j] * db2[i];\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for (int i = 0; i < nn->hidden_size; i++) {\n",
    "        dL_da1[i] = 0.0;\n",
    "        for (int j = 0; j < nn->output_size; j++) {\n",
    "            dL_da1[i] += dL_da2[j] * da2_dz2[j] * nn->w2[i * nn->output_size + j];\n",
    "        }\n",
    "        da1_dz1[i] = relu_derivative(nn->z1[i]);\n",
    "        db1[i] = dL_da1[i] * da1_dz1[i];\n",
    "        for (int j = 0; j < nn->input_size; j++) {\n",
    "            dw1[j * nn->hidden_size + i] = input[j] * db1[i];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Update parameters\n",
    "void update_params(NeuralNetwork *nn, double *dw1, double *db1, double *dw2, double *db2, double learning_rate, int batch_size) {\n",
    "    for (int i = 0; i < nn->input_size * nn->hidden_size; i++) {\n",
    "        nn->w1[i] -= (learning_rate * dw1[i]) / batch_size;\n",
    "    }\n",
    "    for (int i = 0; i < nn->hidden_size; i++) {\n",
    "        nn->b1[i] -= (learning_rate * db1[i]) / batch_size;\n",
    "    }\n",
    "    for (int i = 0; i < nn->hidden_size * nn->output_size; i++) {\n",
    "        nn->w2[i] -= (learning_rate * dw2[i]) / batch_size;\n",
    "    }\n",
    "    for (int i = 0; i < nn->output_size; i++) {\n",
    "        nn->b2[i] -= (learning_rate * db2[i]) / batch_size;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Predict function\n",
    "void predict(NeuralNetwork *nn, double *input, double *output) {\n",
    "    forward(nn, input);\n",
    "    for (int i = 0; i < nn->output_size; i++) {\n",
    "        output[i] = nn->a2[i] > 0.5 ? 1.0 : 0.0;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Test accuracy\n",
    "double test_accuracy(NeuralNetwork *nn, double *x_test, double *y_test, int data_size) {\n",
    "    int correct = 0;\n",
    "    double *output = (double *)malloc(nn->output_size * sizeof(double));\n",
    "    \n",
    "    for (int i = 0; i < data_size; i++) {\n",
    "        double *input = &x_test[i * nn->input_size];\n",
    "        double y_true = y_test[i];\n",
    "        \n",
    "        predict(nn, input, output);\n",
    "        \n",
    "        if (output[0] == y_true) {\n",
    "            correct++;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    free(output);\n",
    "    return (double)correct / data_size;\n",
    "}\n",
    "\n",
    "// Train the network\n",
    "void train(NeuralNetwork *nn, double *x_train, double *y_train, int epochs, double learning_rate, int batch_size, int train_data_size, double **best_w1, double **best_b1, double **best_w2, double **best_b2, double *x_test, double *y_test, int test_data_size) {\n",
    "    double *dw1 = (double *)malloc(nn->input_size * nn->hidden_size * sizeof(double));\n",
    "    double *db1 = (double *)malloc(nn->hidden_size * sizeof(double));\n",
    "    double *dw2 = (double *)malloc(nn->hidden_size * nn->output_size * sizeof(double));\n",
    "    double *db2 = (double *)malloc(nn->output_size * sizeof(double));\n",
    "    double *batch_dw1 = (double *)malloc(nn->input_size * nn->hidden_size * sizeof(double));\n",
    "    double *batch_db1 = (double *)malloc(nn->hidden_size * sizeof(double));\n",
    "    double *batch_dw2 = (double *)malloc(nn->hidden_size * nn->output_size * sizeof(double));\n",
    "    double *batch_db2 = (double *)malloc(nn->output_size * sizeof(double));\n",
    "    \n",
    "    if (!dw1 || !db1 || !dw2 || !db2) {\n",
    "        perror(\"Error allocating memory for gradients\");\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "    \n",
    "    double min_loss = DBL_MAX;\n",
    "    for (int epoch = 0; epoch < epochs; epoch++) {\n",
    "        double epoch_loss = 0.0;\n",
    "        int num_batches = (train_data_size + batch_size - 1) / batch_size; // Ceiling division\n",
    "        \n",
    "        for (int batch = 0; batch < num_batches; batch++) {\n",
    "            int start = batch * batch_size;\n",
    "            int end = start + batch_size < train_data_size ? start + batch_size : train_data_size;\n",
    "            int current_batch_size = end - start;\n",
    "\n",
    "            memset(dw1, 0, nn->input_size * nn->hidden_size * sizeof(double));\n",
    "            memset(db1, 0, nn->hidden_size * sizeof(double));\n",
    "            memset(dw2, 0, nn->hidden_size * nn->output_size * sizeof(double));\n",
    "            memset(db2, 0, nn->output_size * sizeof(double));\n",
    "            \n",
    "            for (int i = start; i < end; i++) {\n",
    "                double *input = x_train + i * nn->input_size;\n",
    "                double *y_true = y_train + i;\n",
    "\n",
    "                forward(nn, input);\n",
    "                epoch_loss += compute_loss(y_true, nn->a2, nn->output_size);\n",
    "                memset(batch_dw1, 0, nn->input_size * nn->hidden_size * sizeof(double));\n",
    "                memset(batch_db1, 0, nn->hidden_size * sizeof(double));\n",
    "                memset(batch_dw2, 0, nn->hidden_size * nn->output_size * sizeof(double));\n",
    "                memset(batch_db2, 0, nn->output_size * sizeof(double));\n",
    "                backward(nn, input, y_true, batch_dw1, batch_db1, batch_dw2, batch_db2);\n",
    "\n",
    "                for (int j = 0; j < nn->input_size * nn->hidden_size; j++) {\n",
    "                    dw1[j] += batch_dw1[j];\n",
    "                }\n",
    "                for (int j = 0; j < nn->hidden_size; j++) {\n",
    "                    db1[j] += batch_db1[j];\n",
    "                }\n",
    "                for (int j = 0; j < nn->hidden_size * nn->output_size; j++) {\n",
    "                    dw2[j] += batch_dw2[j];\n",
    "                }\n",
    "                for (int j = 0; j < nn->output_size; j++) {\n",
    "                    db2[j] += batch_db2[j];\n",
    "                }\n",
    "            }\n",
    "            update_params(nn, dw1, db1, dw2, db2, learning_rate, current_batch_size);\n",
    "        }\n",
    "        \n",
    "        epoch_loss /= train_data_size;\n",
    "        printf(\"Epoch %d, Loss: %.8f Accuracy: %.2f%%\\n\", epoch + 1, epoch_loss, test_accuracy(nn, x_test, y_test, NUM_TEST_IMAGES) * 100.0);\n",
    "\n",
    "        if (epoch_loss < min_loss) {\n",
    "            min_loss = epoch_loss;\n",
    "            memcpy(*best_w1, nn->w1, nn->input_size * nn->hidden_size * sizeof(double));\n",
    "            memcpy(*best_b1, nn->b1, nn->hidden_size * sizeof(double));\n",
    "            memcpy(*best_w2, nn->w2, nn->hidden_size * nn->output_size * sizeof(double));\n",
    "            memcpy(*best_b2, nn->b2, nn->output_size * sizeof(double));\n",
    "        }\n",
    "    }\n",
    "\n",
    "    free(dw1);\n",
    "    free(db1);\n",
    "    free(dw2);\n",
    "    free(db2);\n",
    "}\n",
    "\n",
    "\n",
    "// Utility function to read CSV files\n",
    "void read_csv(const char *filename, double *data, int rows, int cols) {\n",
    "    FILE *file = fopen(filename, \"r\");\n",
    "    if (!file) {\n",
    "        perror(\"Error opening file\");\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "\n",
    "    for (int i = 0; i < rows; i++) {\n",
    "        for (int j = 0; j < cols; j++) {\n",
    "            if (fscanf(file, \"%lf,\", &data[i * cols + j]) != 1) {\n",
    "                perror(\"Error reading file\");\n",
    "                fclose(file);\n",
    "                exit(EXIT_FAILURE);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    fclose(file);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    srand(12);\n",
    "\n",
    "    double *train_data = (double *)malloc(NUM_TRAIN_IMAGES * FLATTENED_SIZE * sizeof(double));\n",
    "    double *train_labels = (double *)malloc(NUM_TRAIN_IMAGES * sizeof(double));\n",
    "    double *test_data = (double *)malloc(NUM_TEST_IMAGES * FLATTENED_SIZE * sizeof(double));\n",
    "    double *test_labels = (double *)malloc(NUM_TEST_IMAGES * sizeof(double));\n",
    "\n",
    "    if (!train_data || !train_labels || !test_data || !test_labels) {\n",
    "        perror(\"Error allocating memory for data\");\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "\n",
    "    read_csv(\"/kaggle/input/catfinal/final_train_images_float32.csv\", train_data, NUM_TRAIN_IMAGES, FLATTENED_SIZE);\n",
    "    read_csv(\"/kaggle/input/catfinal/final_train_labels_float32.csv\", train_labels, NUM_TRAIN_IMAGES, 1);\n",
    "    read_csv(\"/kaggle/input/catfinal/test_images_float32.csv\", test_data, NUM_TEST_IMAGES, FLATTENED_SIZE);\n",
    "    read_csv(\"/kaggle/input/catfinal/test_labels_float32.csv\", test_labels, NUM_TEST_IMAGES, 1);\n",
    "\n",
    "    // Normalize the data\n",
    "    for (int i = 0; i < NUM_TRAIN_IMAGES * FLATTENED_SIZE; i++) {\n",
    "        train_data[i] /= 255.0;\n",
    "    }\n",
    "    for (int i = 0; i < NUM_TEST_IMAGES * FLATTENED_SIZE; i++) {\n",
    "        test_data[i] /= 255.0;\n",
    "    }\n",
    "\n",
    "    int input_size = FLATTENED_SIZE;\n",
    "    int hidden_size = 128;\n",
    "    int output_size = 1;\n",
    "\n",
    "    NeuralNetwork nn;\n",
    "    init_nn(&nn, input_size, hidden_size, output_size);\n",
    "\n",
    "    double *best_w1 = (double *)malloc(input_size * hidden_size * sizeof(double));\n",
    "    double *best_b1 = (double *)malloc(hidden_size * sizeof(double));\n",
    "    double *best_w2 = (double *)malloc(hidden_size * output_size * sizeof(double));\n",
    "    double *best_b2 = (double *)malloc(output_size * sizeof(double));\n",
    "\n",
    "    if (!best_w1 || !best_b1 || !best_w2 || !best_b2) {\n",
    "        perror(\"Error allocating memory for best parameters\");\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "\n",
    "    train(&nn, train_data, train_labels, 100, 0.01, BATCH_SIZE, NUM_TRAIN_IMAGES, &best_w1, &best_b1, &best_w2, &best_b2, test_data, test_labels, NUM_TEST_IMAGES);\n",
    "\n",
    "    // Load the best parameters into the network\n",
    "    memcpy(nn.w1, best_w1, input_size * hidden_size * sizeof(double));\n",
    "    memcpy(nn.b1, best_b1, hidden_size * sizeof(double));\n",
    "    memcpy(nn.w2, best_w2, hidden_size * output_size * sizeof(double));\n",
    "    memcpy(nn.b2, best_b2, output_size * sizeof(double));\n",
    "\n",
    "    // Free allocated memory\n",
    "    free(nn.w1);\n",
    "    free(nn.b1);\n",
    "    free(nn.w2);\n",
    "    free(nn.b2);\n",
    "    free(nn.z1);\n",
    "    free(nn.a1);\n",
    "    free(nn.z2);\n",
    "    free(nn.a2);\n",
    "    free(train_data);\n",
    "    free(train_labels);\n",
    "    free(test_data);\n",
    "    free(test_labels);\n",
    "    free(best_w1);\n",
    "    free(best_b1);\n",
    "    free(best_w2);\n",
    "    free(best_b2);\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ae0e8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T15:32:36.622479Z",
     "iopub.status.busy": "2024-06-23T15:32:36.622224Z",
     "iopub.status.idle": "2024-06-23T15:32:37.902051Z",
     "shell.execute_reply": "2024-06-23T15:32:37.900767Z"
    },
    "papermill": {
     "duration": 1.286529,
     "end_time": "2024-06-23T15:32:37.904432",
     "exception": false,
     "start_time": "2024-06-23T15:32:36.617903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcc -o model_cpu model_cpu.c -lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b095d90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T15:32:37.913298Z",
     "iopub.status.busy": "2024-06-23T15:32:37.912950Z",
     "iopub.status.idle": "2024-06-23T15:51:04.044843Z",
     "shell.execute_reply": "2024-06-23T15:51:04.043839Z"
    },
    "papermill": {
     "duration": 1106.138703,
     "end_time": "2024-06-23T15:51:04.046947",
     "exception": false,
     "start_time": "2024-06-23T15:32:37.908244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.94437852 Accuracy: 56.00%\r\n",
      "Epoch 2, Loss: 0.70524541 Accuracy: 34.00%\r\n",
      "Epoch 3, Loss: 0.65015862 Accuracy: 34.00%\r\n",
      "Epoch 4, Loss: 0.63600978 Accuracy: 34.00%\r\n",
      "Epoch 5, Loss: 0.63023589 Accuracy: 34.00%\r\n",
      "Epoch 6, Loss: 0.62374930 Accuracy: 34.00%\r\n",
      "Epoch 7, Loss: 0.61747006 Accuracy: 34.00%\r\n",
      "Epoch 8, Loss: 0.61361335 Accuracy: 34.00%\r\n",
      "Epoch 9, Loss: 0.60615193 Accuracy: 34.00%\r\n",
      "Epoch 10, Loss: 0.60142310 Accuracy: 36.00%\r\n",
      "Epoch 11, Loss: 0.59791794 Accuracy: 36.00%\r\n",
      "Epoch 12, Loss: 0.59284174 Accuracy: 36.00%\r\n",
      "Epoch 13, Loss: 0.59073313 Accuracy: 36.00%\r\n",
      "Epoch 14, Loss: 0.58852557 Accuracy: 36.00%\r\n",
      "Epoch 15, Loss: 0.58294463 Accuracy: 36.00%\r\n",
      "Epoch 16, Loss: 0.57848187 Accuracy: 40.00%\r\n",
      "Epoch 17, Loss: 0.57185822 Accuracy: 44.00%\r\n",
      "Epoch 18, Loss: 0.57231915 Accuracy: 46.00%\r\n",
      "Epoch 19, Loss: 0.56606038 Accuracy: 40.00%\r\n",
      "Epoch 20, Loss: 0.56186431 Accuracy: 46.00%\r\n",
      "Epoch 21, Loss: 0.55685367 Accuracy: 50.00%\r\n",
      "Epoch 22, Loss: 0.55751762 Accuracy: 50.00%\r\n",
      "Epoch 23, Loss: 0.55476107 Accuracy: 44.00%\r\n",
      "Epoch 24, Loss: 0.54472264 Accuracy: 54.00%\r\n",
      "Epoch 25, Loss: 0.54477724 Accuracy: 54.00%\r\n",
      "Epoch 26, Loss: 0.54354569 Accuracy: 50.00%\r\n",
      "Epoch 27, Loss: 0.53777443 Accuracy: 54.00%\r\n",
      "Epoch 28, Loss: 0.53273667 Accuracy: 50.00%\r\n",
      "Epoch 29, Loss: 0.52259798 Accuracy: 54.00%\r\n",
      "Epoch 30, Loss: 0.52718059 Accuracy: 54.00%\r\n",
      "Epoch 31, Loss: 0.52284048 Accuracy: 58.00%\r\n",
      "Epoch 32, Loss: 0.52261418 Accuracy: 54.00%\r\n",
      "Epoch 33, Loss: 0.51260888 Accuracy: 56.00%\r\n",
      "Epoch 34, Loss: 0.51622503 Accuracy: 56.00%\r\n",
      "Epoch 35, Loss: 0.50549322 Accuracy: 58.00%\r\n",
      "Epoch 36, Loss: 0.50838553 Accuracy: 60.00%\r\n",
      "Epoch 37, Loss: 0.49992691 Accuracy: 64.00%\r\n",
      "Epoch 38, Loss: 0.50408956 Accuracy: 58.00%\r\n",
      "Epoch 39, Loss: 0.49598605 Accuracy: 64.00%\r\n",
      "Epoch 40, Loss: 0.49685836 Accuracy: 60.00%\r\n",
      "Epoch 41, Loss: 0.48545657 Accuracy: 60.00%\r\n",
      "Epoch 42, Loss: 0.48705711 Accuracy: 64.00%\r\n",
      "Epoch 43, Loss: 0.48709698 Accuracy: 68.00%\r\n",
      "Epoch 44, Loss: 0.48558701 Accuracy: 64.00%\r\n",
      "Epoch 45, Loss: 0.48506709 Accuracy: 66.00%\r\n",
      "Epoch 46, Loss: 0.47500827 Accuracy: 70.00%\r\n",
      "Epoch 47, Loss: 0.47716211 Accuracy: 64.00%\r\n",
      "Epoch 48, Loss: 0.46660424 Accuracy: 62.00%\r\n",
      "Epoch 49, Loss: 0.45778110 Accuracy: 70.00%\r\n",
      "Epoch 50, Loss: 0.46555514 Accuracy: 72.00%\r\n",
      "Epoch 51, Loss: 0.46766982 Accuracy: 72.00%\r\n",
      "Epoch 52, Loss: 0.46948220 Accuracy: 74.00%\r\n",
      "Epoch 53, Loss: 0.46869165 Accuracy: 76.00%\r\n",
      "Epoch 54, Loss: 0.47067100 Accuracy: 76.00%\r\n",
      "Epoch 55, Loss: 0.47211473 Accuracy: 70.00%\r\n",
      "Epoch 56, Loss: 0.44027270 Accuracy: 64.00%\r\n",
      "Epoch 57, Loss: 0.43479839 Accuracy: 68.00%\r\n",
      "Epoch 58, Loss: 0.43305710 Accuracy: 74.00%\r\n",
      "Epoch 59, Loss: 0.43477178 Accuracy: 72.00%\r\n",
      "Epoch 60, Loss: 0.43563706 Accuracy: 76.00%\r\n",
      "Epoch 61, Loss: 0.44057064 Accuracy: 76.00%\r\n",
      "Epoch 62, Loss: 0.44373839 Accuracy: 76.00%\r\n",
      "Epoch 63, Loss: 0.44644546 Accuracy: 78.00%\r\n",
      "Epoch 64, Loss: 0.43919821 Accuracy: 76.00%\r\n",
      "Epoch 65, Loss: 0.43093125 Accuracy: 76.00%\r\n",
      "Epoch 66, Loss: 0.42180950 Accuracy: 76.00%\r\n",
      "Epoch 67, Loss: 0.42690676 Accuracy: 78.00%\r\n",
      "Epoch 68, Loss: 0.42397586 Accuracy: 76.00%\r\n",
      "Epoch 69, Loss: 0.41856804 Accuracy: 78.00%\r\n",
      "Epoch 70, Loss: 0.40309069 Accuracy: 76.00%\r\n",
      "Epoch 71, Loss: 0.40113854 Accuracy: 78.00%\r\n",
      "Epoch 72, Loss: 0.40312047 Accuracy: 78.00%\r\n",
      "Epoch 73, Loss: 0.40150610 Accuracy: 76.00%\r\n",
      "Epoch 74, Loss: 0.40419291 Accuracy: 80.00%\r\n",
      "Epoch 75, Loss: 0.40609242 Accuracy: 76.00%\r\n",
      "Epoch 76, Loss: 0.41083649 Accuracy: 76.00%\r\n",
      "Epoch 77, Loss: 0.41442337 Accuracy: 78.00%\r\n",
      "Epoch 78, Loss: 0.39671931 Accuracy: 78.00%\r\n",
      "Epoch 79, Loss: 0.38597883 Accuracy: 78.00%\r\n",
      "Epoch 80, Loss: 0.37904292 Accuracy: 78.00%\r\n",
      "Epoch 81, Loss: 0.37731357 Accuracy: 80.00%\r\n",
      "Epoch 82, Loss: 0.37192994 Accuracy: 78.00%\r\n",
      "Epoch 83, Loss: 0.36568066 Accuracy: 78.00%\r\n",
      "Epoch 84, Loss: 0.36130785 Accuracy: 78.00%\r\n",
      "Epoch 85, Loss: 0.35995879 Accuracy: 78.00%\r\n",
      "Epoch 86, Loss: 0.36088162 Accuracy: 80.00%\r\n",
      "Epoch 87, Loss: 0.37004559 Accuracy: 76.00%\r\n",
      "Epoch 88, Loss: 0.39321964 Accuracy: 78.00%\r\n",
      "Epoch 89, Loss: 0.39919211 Accuracy: 80.00%\r\n",
      "Epoch 90, Loss: 0.35591615 Accuracy: 78.00%\r\n",
      "Epoch 91, Loss: 0.34851778 Accuracy: 80.00%\r\n",
      "Epoch 92, Loss: 0.34364304 Accuracy: 78.00%\r\n",
      "Epoch 93, Loss: 0.34262565 Accuracy: 80.00%\r\n",
      "Epoch 94, Loss: 0.34228946 Accuracy: 80.00%\r\n",
      "Epoch 95, Loss: 0.34307715 Accuracy: 78.00%\r\n",
      "Epoch 96, Loss: 0.35388356 Accuracy: 78.00%\r\n",
      "Epoch 97, Loss: 0.37762504 Accuracy: 78.00%\r\n",
      "Epoch 98, Loss: 0.38662101 Accuracy: 78.00%\r\n",
      "Epoch 99, Loss: 0.35409701 Accuracy: 80.00%\r\n",
      "Epoch 100, Loss: 0.34421810 Accuracy: 78.00%\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!./model_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17822de0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T15:51:04.072009Z",
     "iopub.status.busy": "2024-06-23T15:51:04.071675Z",
     "iopub.status.idle": "2024-06-23T15:51:04.088813Z",
     "shell.execute_reply": "2024-06-23T15:51:04.088030Z"
    },
    "papermill": {
     "duration": 0.03202,
     "end_time": "2024-06-23T15:51:04.090706",
     "exception": false,
     "start_time": "2024-06-23T15:51:04.058686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_gpu.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_gpu.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "#include <float.h>\n",
    "#include <time.h>\n",
    "#include <string.h>\n",
    "\n",
    "#define NUM_TRAIN_IMAGES 200\n",
    "#define NUM_TEST_IMAGES 50\n",
    "#define IMG_HEIGHT 64\n",
    "#define IMG_WIDTH 64\n",
    "#define IMG_DEPTH 3\n",
    "#define FLATTENED_SIZE (IMG_HEIGHT * IMG_WIDTH * IMG_DEPTH)\n",
    "#define NUM_CLASSES 1\n",
    "#define BATCH_SIZE 100\n",
    "\n",
    "#define EPSILON 1e-10\n",
    "\n",
    "// Activation functions\n",
    "__device__ double sigmoid(double x) {\n",
    "    return 1.0 / (1.0 + exp(-x));\n",
    "}\n",
    "\n",
    "__device__ double sigmoid_derivative(double x) {\n",
    "    double sx = sigmoid(x);\n",
    "    return sx * (1 - sx);\n",
    "}\n",
    "\n",
    "__device__ double relu(double x) {\n",
    "    return x > 0 ? x : 0;\n",
    "}\n",
    "\n",
    "__device__ double relu_derivative(double x) {\n",
    "    return x > 0 ? 1 : 0;\n",
    "}\n",
    "\n",
    "__device__ double stable_sigmoid(double x) {\n",
    "    double sig = sigmoid(x);\n",
    "    return fmax(fmin(sig, 1.0 - EPSILON), EPSILON);\n",
    "}\n",
    "\n",
    "// Compute loss\n",
    "__device__ double compute_loss(double y_true, double y_pred) {\n",
    "    y_pred = fmin(fmax(y_pred, EPSILON), 1.0 - EPSILON);\n",
    "    double term1 = y_true * log(y_pred);\n",
    "    double term2 = (1 - y_true) * log(1 - y_pred);\n",
    "    return -(term1 + term2);\n",
    "}\n",
    "\n",
    "__device__ double compute_derivative_loss(double y_true, double y_pred) {\n",
    "    double term1 = y_true / y_pred;\n",
    "    double term2 = - (1 - y_true) / (1 - y_pred);\n",
    "    return - (term1 + term2);\n",
    "}\n",
    "\n",
    "// Host version of compute_loss\n",
    "double host_compute_loss(double y_true, double y_pred) {\n",
    "    y_pred = fmin(fmax(y_pred, EPSILON), 1.0 - EPSILON);\n",
    "    double term1 = y_true * log(y_pred);\n",
    "    double term2 = (1 - y_true) * log(1 - y_pred);\n",
    "    return -(term1 + term2);\n",
    "}\n",
    "\n",
    "double random_normal() {\n",
    "    double u1 = (double)rand() / RAND_MAX;\n",
    "    double u2 = (double)rand() / RAND_MAX;\n",
    "    return sqrt(-2.0 * log(u1)) * cos(2.0 * M_PI * u2);\n",
    "}\n",
    "\n",
    "void he_init(double *weights, int fan_in, int fan_out) {\n",
    "    double stddev = sqrt(2.0 / fan_in);\n",
    "    for (int i = 0; i < fan_in * fan_out; i++) {\n",
    "        weights[i] = random_normal() * stddev;\n",
    "    }\n",
    "}\n",
    "\n",
    "typedef struct {\n",
    "    int input_size;\n",
    "    int hidden_size;\n",
    "    int output_size;\n",
    "    double *w1;\n",
    "    double *b1;\n",
    "    double *w2;\n",
    "    double *b2;\n",
    "    double *z1;\n",
    "    double *a1;\n",
    "    double *z2;\n",
    "    double *a2;\n",
    "} NeuralNetwork;\n",
    "\n",
    "// Initialize the neural network\n",
    "void init_nn(NeuralNetwork *nn, int input_size, int hidden_size, int output_size) {\n",
    "    nn->input_size = input_size;\n",
    "    nn->hidden_size = hidden_size;\n",
    "    nn->output_size = output_size;\n",
    "\n",
    "    nn->w1 = (double*)malloc(input_size * hidden_size * sizeof(double));\n",
    "    nn->b1 = (double*)malloc(hidden_size * sizeof(double));\n",
    "    nn->w2 = (double*)malloc(hidden_size * output_size * sizeof(double));\n",
    "    nn->b2 = (double*)malloc(output_size * sizeof(double));\n",
    "\n",
    "    he_init(nn->w1, input_size, hidden_size);\n",
    "    he_init(nn->w2, hidden_size, output_size);\n",
    "}\n",
    "\n",
    "void free_nn(NeuralNetwork *nn) {\n",
    "    free(nn->w1);\n",
    "    free(nn->b1);\n",
    "    free(nn->w2);\n",
    "    free(nn->b2);\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "// Forward propagation\n",
    "__global__ void forward_layer1(double *gpu_input, double *gpu_z1, double *gpu_a1, double *gpu_w1, double *gpu_b1, int batch_size, int input_size, int hidden_size) {\n",
    "    int Row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int Col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (Row < batch_size && Col < hidden_size) {\n",
    "        double sum1 = 0.0;\n",
    "        for (int k = 0; k < input_size; k++) {\n",
    "            sum1 += gpu_input[Row * input_size + k] * gpu_w1[k * hidden_size + Col];\n",
    "        }\n",
    "        gpu_z1[Row * hidden_size + Col] = sum1 + gpu_b1[Col];\n",
    "        \n",
    "        gpu_a1[Row * hidden_size + Col] = relu(gpu_z1[Row * hidden_size + Col]);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void forward_layer2(double *gpu_a1, double *gpu_z2, double *gpu_a2, double *gpu_w2, double *gpu_b2, int batch_size, int hidden_size, int output_size) {\n",
    "    int Row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int Col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (Row < batch_size && Col < output_size) {\n",
    "        double sum2 = 0.0;\n",
    "        for (int k = 0; k < hidden_size; k++) {\n",
    "            sum2 += gpu_a1[Row * hidden_size + k] * gpu_w2[k * output_size + Col];\n",
    "        }\n",
    "        gpu_z2[Row * output_size + Col] = sum2 + gpu_b2[Col];\n",
    "        gpu_a2[Row * output_size + Col] = stable_sigmoid(gpu_z2[Row * output_size + Col]);\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "// Backward propagation\n",
    "__global__ void backward1(double *gpu_loss, double *gpu_dLa2, double *gpu_da2z2, double *gpu_db2, double *gpu_y_true, double *gpu_z2, double *gpu_a2, int batch_size, int output_size) {\n",
    "    int Row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int Col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (Row < batch_size && Col < output_size) {\n",
    "        gpu_loss[Row * output_size + Col] = compute_loss(gpu_y_true[Row * output_size + Col], gpu_a2[Row * output_size + Col]);\n",
    "        gpu_dLa2[Row * output_size + Col] = compute_derivative_loss(gpu_y_true[Row * output_size + Col], gpu_a2[Row * output_size + Col]);\n",
    "        gpu_da2z2[Row * output_size + Col] = sigmoid_derivative(gpu_z2[Row * output_size + Col] );\n",
    "        gpu_db2[Row * output_size + Col] = gpu_dLa2[Row * output_size + Col] * gpu_da2z2[Row * output_size + Col];\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void backward2(double *gpu_dw2, double *gpu_dLa1, double *gpu_da1z1, double *gpu_db1, double *gpu_dw1, double *gpu_input, double *gpu_z1, double *gpu_a1, double *gpu_w2, double *gpu_db2, int batch_size, int input_size, int hidden_size) {\n",
    "    int Row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int Col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    \n",
    "       if (Row < batch_size && Col < hidden_size) {\n",
    "            gpu_dw2[Row * hidden_size + Col] = gpu_db2[Row] * gpu_a1[Row * hidden_size + Col];\n",
    "            \n",
    "            gpu_dLa1[Row * hidden_size + Col] = gpu_db2[Row] * gpu_w2[Col];\n",
    "            gpu_da1z1[Row * hidden_size + Col] = relu_derivative(gpu_z1[Row * hidden_size + Col]);\n",
    "            gpu_db1[Row * hidden_size + Col] = gpu_dLa1[Row * hidden_size + Col] * gpu_da1z1[Row * hidden_size + Col];\n",
    "\n",
    "         for (int i = 0; i < input_size; i++) {\n",
    "                gpu_dw1[Row * input_size * hidden_size + i * hidden_size + Col] = gpu_input[Row * input_size + i]  * gpu_db1[Row * hidden_size + Col] ;\n",
    "            } \n",
    "       }\n",
    "}\n",
    "\n",
    "// Compute loss and gradient for updating\n",
    "__global__ void reduction(\n",
    "    double *gpu_dw2, double *gpu_db1, double *gpu_dw1, \n",
    "    double *gpu_w2, double *gpu_b1, double *gpu_w1, \n",
    "    double *gpu_loss, double *gpu_db2, double *gpu_epoch_loss, double *gpu_b2,\n",
    "    int batch_size, int input_size, int hidden_size, double learning_rate) {\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int node = blockIdx.x;\n",
    "\n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) {\n",
    "            if (tid + s >= 100) {\n",
    "                if (node == 0) {\n",
    "                    gpu_loss[tid] += 0;\n",
    "                    gpu_db2[tid] += 0;\n",
    "                }\n",
    "\n",
    "                gpu_dw2[tid * hidden_size + node] += 0;\n",
    "                gpu_db1[tid * hidden_size + node] += 0;\n",
    "                for (int i = 0; i < input_size; i++) {\n",
    "                    gpu_dw1[tid * input_size * hidden_size + i * hidden_size + node] += 0;\n",
    "                }\n",
    "            } else {\n",
    "                if (node == 0) {\n",
    "                    gpu_loss[tid] += gpu_loss[tid + s];\n",
    "                    gpu_db2[tid] += gpu_db2[tid + s];\n",
    "                }\n",
    "                gpu_dw2[tid * hidden_size + node] += gpu_dw2[(tid + s) * hidden_size + node];\n",
    "                gpu_db1[tid * hidden_size + node] += gpu_db1[(tid + s) * hidden_size + node];\n",
    "                for (int i = 0; i < input_size; i++) {\n",
    "                    gpu_dw1[tid * input_size * hidden_size + i * hidden_size + node] += gpu_dw1[(tid + s) * input_size * hidden_size + i * hidden_size + node];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    if (tid == 0) {\n",
    "        if (node == 0) {\n",
    "            gpu_db2[0] /= batch_size;\n",
    "            *gpu_epoch_loss += gpu_loss[0];\n",
    "            gpu_b2[0] -= learning_rate * gpu_db2[0];\n",
    "        }\n",
    "        gpu_dw2[node] /= batch_size;\n",
    "        gpu_db1[node] /= batch_size;\n",
    "        gpu_w2[node] -= learning_rate * gpu_dw2[node];\n",
    "        gpu_b1[node] -= learning_rate * gpu_db1[node];\n",
    "        for (int i = 0; i < input_size; i++) {\n",
    "            gpu_dw1[i * hidden_size + node] /= batch_size;\n",
    "            gpu_w1[i * hidden_size + node] -= learning_rate * gpu_dw1[i * hidden_size + node];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Predict function\n",
    "void predict(NeuralNetwork *nn, double *input, double *output, double *gpu_input, double *gpu_z1, double *gpu_a1, double *gpu_z2, double *gpu_a2, double *gpu_w1, double *gpu_b1, double *gpu_w2, double *gpu_b2) {\n",
    "    dim3 gridSize(16, 16);\n",
    "    dim3 blockSize(16, 16);\n",
    "    forward_layer1<<<gridSize, blockSize>>>(gpu_input, gpu_z1, gpu_a1, gpu_w1, gpu_b1, BATCH_SIZE, nn->input_size, nn->hidden_size);\n",
    "    cudaDeviceSynchronize();\n",
    "    forward_layer2<<<gridSize, blockSize>>>(gpu_a1, gpu_z2, gpu_a2, gpu_w2, gpu_b2, BATCH_SIZE, nn->hidden_size, nn->output_size);\n",
    "    cudaDeviceSynchronize();\n",
    "    cudaMemcpy(output, gpu_a2, nn->output_size * sizeof(double), cudaMemcpyDeviceToHost);\n",
    "}\n",
    "\n",
    "// Test accuracy\n",
    "double test_accuracy(NeuralNetwork *nn, double *x_test, double *y_test, int data_size, double *gpu_input, double *gpu_z1, double *gpu_a1, double *gpu_z2, double *gpu_a2, double *gpu_w1, double *gpu_b1, double *gpu_w2, double *gpu_b2) {\n",
    "    int correct = 0;\n",
    "    double *output = (double*)malloc(nn->output_size * sizeof(double));\n",
    "\n",
    "    for (int i = 0; i < data_size; i++) {\n",
    "        double *input = &x_test[i * nn->input_size];\n",
    "        double y_true = y_test[i];\n",
    "\n",
    "        cudaMemcpy(gpu_input, input, nn->input_size * sizeof(double), cudaMemcpyHostToDevice);\n",
    "        predict(nn, input, output, gpu_input, gpu_z1, gpu_a1, gpu_z2, gpu_a2, gpu_w1, gpu_b1, gpu_w2, gpu_b2);\n",
    "\n",
    "        if ((output[0] > 0.5) == y_true) {\n",
    "            correct++;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    free(output);\n",
    "    return (double)correct / data_size;\n",
    "}\n",
    "\n",
    "// Train the network\n",
    "void train(NeuralNetwork *nn, double *x_train, double* y_train, int epochs, double learning_rate, int batch_size, int train_data_size, double *x_test, double *y_test, int test_data_size) {\n",
    "    double *gpu_input, *gpu_z1, *gpu_a1, *gpu_z2, *gpu_a2, *gpu_w1, *gpu_b1, *gpu_w2, *gpu_b2, *gpu_dw1, *gpu_db1, *gpu_dw2, *gpu_db2;\n",
    "    double *gpu_epoch_loss, *gpu_loss, *gpu_dLa2, *gpu_da2z2, *gpu_dLa1, *gpu_da1z1, *gpu_y_true;\n",
    "\n",
    "    cudaMalloc(&gpu_input, BATCH_SIZE * nn->input_size * sizeof(double));\n",
    "    cudaMalloc(&gpu_z1, BATCH_SIZE * nn->hidden_size * sizeof(double));\n",
    "    cudaMalloc(&gpu_a1, BATCH_SIZE * nn->hidden_size * sizeof(double));\n",
    "    cudaMalloc(&gpu_z2, BATCH_SIZE * nn->output_size * sizeof(double));\n",
    "    cudaMalloc(&gpu_a2, BATCH_SIZE * nn->output_size * sizeof(double));\n",
    "    cudaMalloc(&gpu_w1, nn->input_size * nn->hidden_size * sizeof(double));\n",
    "    cudaMalloc(&gpu_b1, nn->hidden_size * sizeof(double));\n",
    "    cudaMalloc(&gpu_w2, nn->hidden_size * nn->output_size * sizeof(double));\n",
    "    cudaMalloc(&gpu_b2, nn->output_size * sizeof(double));\n",
    "    cudaMalloc(&gpu_dw1, BATCH_SIZE * nn->input_size * nn->hidden_size * sizeof(double));\n",
    "    cudaMalloc(&gpu_db1, BATCH_SIZE * nn->hidden_size * sizeof(double));\n",
    "    cudaMalloc(&gpu_dw2, BATCH_SIZE * nn->hidden_size * nn->output_size * sizeof(double));\n",
    "    cudaMalloc(&gpu_db2, BATCH_SIZE * nn->output_size * sizeof(double));\n",
    "    \n",
    "    \n",
    "    cudaMemcpy(gpu_w1, nn->w1, nn->input_size * nn->hidden_size * sizeof(double), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(gpu_b1, nn->b1, nn->hidden_size * sizeof(double), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(gpu_w2, nn->w2, nn->hidden_size * nn->output_size * sizeof(double), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(gpu_b2, nn->b2, nn->output_size * sizeof(double), cudaMemcpyHostToDevice);\n",
    "\n",
    "    cudaMalloc(&gpu_loss, BATCH_SIZE * nn->output_size * sizeof(double));\n",
    "    cudaMalloc(&gpu_dLa2, BATCH_SIZE * nn->output_size * sizeof(double));\n",
    "    cudaMalloc(&gpu_da2z2, BATCH_SIZE * nn->output_size * sizeof(double));\n",
    "    cudaMalloc(&gpu_dLa1, BATCH_SIZE * nn->hidden_size * sizeof(double));\n",
    "    cudaMalloc(&gpu_da1z1, BATCH_SIZE * nn->hidden_size * sizeof(double));\n",
    "    cudaMalloc(&gpu_y_true, BATCH_SIZE * sizeof(double));\n",
    "    cudaMalloc(&gpu_epoch_loss, sizeof(double));\n",
    "    \n",
    "    double epoch_loss;\n",
    "\n",
    "    //int no = (NUM_TRAIN_IMAGES + BATCH_SIZE - 1) / BATCH_SIZE;\n",
    "    \n",
    "    dim3 gridSize(16, 16);\n",
    "    dim3 blockSize(16, 16);\n",
    "\n",
    "    for (int epoch = 0; epoch < epochs; epoch++) {\n",
    "        epoch_loss = 0.0;\n",
    "        cudaMemcpy(gpu_epoch_loss, &epoch_loss, sizeof(double), cudaMemcpyHostToDevice);\n",
    "        \n",
    "        int num_batches = (train_data_size + batch_size - 1) / batch_size;  // Ceiling division\n",
    "\n",
    "        for (int batch = 0; batch < num_batches; batch++) {\n",
    "            int start = batch * batch_size;\n",
    "            int end = start + batch_size < train_data_size ? start + batch_size : train_data_size;\n",
    "            int current_batch_size = end - start;\n",
    "\n",
    "            double *input = x_train + start * nn->input_size;\n",
    "            double *y_true = y_train + start;\n",
    "\n",
    "            cudaMemcpy(gpu_input, input, current_batch_size * nn->input_size * sizeof(double), cudaMemcpyHostToDevice);\n",
    "            cudaMemcpy(gpu_y_true, y_true, current_batch_size * sizeof(double), cudaMemcpyHostToDevice);\n",
    "\n",
    "            forward_layer1<<<gridSize, blockSize>>>(gpu_input, gpu_z1, gpu_a1, gpu_w1, gpu_b1, current_batch_size, nn->input_size, nn->hidden_size);\n",
    "            cudaDeviceSynchronize();\n",
    "\n",
    "            forward_layer2<<<gridSize, blockSize>>>(gpu_a1, gpu_z2, gpu_a2, gpu_w2, gpu_b2, current_batch_size, nn->hidden_size, nn->output_size);\n",
    "            cudaDeviceSynchronize();\n",
    "            \n",
    "            backward1<<<gridSize, blockSize>>>(gpu_loss, gpu_dLa2, gpu_da2z2, gpu_db2, gpu_y_true, gpu_z2, gpu_a2, current_batch_size, nn->output_size);\n",
    "            cudaDeviceSynchronize();\n",
    "            \n",
    "            backward2<<<gridSize, blockSize>>>(gpu_dw2, gpu_dLa1, gpu_da1z1, gpu_db1, gpu_dw1, gpu_input, gpu_z1, gpu_a1, gpu_w2, gpu_db2, current_batch_size, nn->input_size, nn->hidden_size);\n",
    "            cudaDeviceSynchronize();\n",
    "            \n",
    "            reduction<<<nn->hidden_size, 128>>>(gpu_dw2, gpu_db1, gpu_dw1, gpu_w2, gpu_b1, gpu_w1, gpu_loss, gpu_db2, gpu_epoch_loss, gpu_b2, current_batch_size, nn->input_size, nn->hidden_size, learning_rate);\n",
    "        \n",
    "        }\n",
    "        cudaMemcpy(&epoch_loss, gpu_epoch_loss, sizeof(double), cudaMemcpyDeviceToHost);\n",
    "\n",
    "        epoch_loss /= NUM_TRAIN_IMAGES;\n",
    "\n",
    "        printf(\"Epoch %d, Loss: %.8f Accuracy: %.2f%%\\n\", epoch + 1, epoch_loss, test_accuracy(nn, x_test, y_test, NUM_TEST_IMAGES, gpu_input, gpu_z1, gpu_a1, gpu_z2, gpu_a2, gpu_w1, gpu_b1, gpu_w2, gpu_b2) * 100.0);\n",
    "    }\n",
    "\n",
    "    cudaFree(gpu_input);\n",
    "    cudaFree(gpu_z1);\n",
    "    cudaFree(gpu_a1);\n",
    "    cudaFree(gpu_z2);\n",
    "    cudaFree(gpu_a2);\n",
    "    cudaFree(gpu_w1);\n",
    "    cudaFree(gpu_b1);\n",
    "    cudaFree(gpu_w2);\n",
    "    cudaFree(gpu_b2);\n",
    "    cudaFree(gpu_dw1);\n",
    "    cudaFree(gpu_db1);\n",
    "    cudaFree(gpu_dw2);\n",
    "    cudaFree(gpu_db2);\n",
    "    cudaFree(gpu_loss);\n",
    "    cudaFree(gpu_dLa2);\n",
    "    cudaFree(gpu_da2z2);\n",
    "    cudaFree(gpu_dLa1);\n",
    "    cudaFree(gpu_da1z1);\n",
    "    cudaFree(gpu_y_true);\n",
    "    cudaFree(gpu_epoch_loss);\n",
    "}\n",
    "\n",
    "\n",
    "// Utility function to read CSV files\n",
    "void read_csv(const char *filename, double *data, int rows, int cols) {\n",
    "    FILE *file = fopen(filename, \"r\");\n",
    "    if (!file) {\n",
    "        perror(\"Error opening file\");\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "\n",
    "    for (int i = 0; i < rows; i++) {\n",
    "        for (int j = 0; j < cols; j++) {\n",
    "            if (fscanf(file, \"%lf,\", &data[i * cols + j]) != 1) {\n",
    "                perror(\"Error reading file\");\n",
    "                fclose(file);\n",
    "                exit(EXIT_FAILURE);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    fclose(file);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    srand(12);\n",
    "\n",
    "    double *train_data = (double*)malloc(NUM_TRAIN_IMAGES * FLATTENED_SIZE * sizeof(double));\n",
    "    double *train_labels = (double*)malloc(NUM_TRAIN_IMAGES * sizeof(double));\n",
    "    double *test_data = (double*)malloc(NUM_TEST_IMAGES * FLATTENED_SIZE * sizeof(double));\n",
    "    double *test_labels = (double*)malloc(NUM_TEST_IMAGES * sizeof(double));\n",
    "\n",
    "    read_csv(\"/kaggle/input/catfinal/final_train_images_float32.csv\", train_data, NUM_TRAIN_IMAGES, FLATTENED_SIZE);\n",
    "    read_csv(\"/kaggle/input/catfinal/final_train_labels_float32.csv\", train_labels, NUM_TRAIN_IMAGES, 1);\n",
    "    read_csv(\"/kaggle/input/catfinal/test_images_float32.csv\", test_data, NUM_TEST_IMAGES, FLATTENED_SIZE);\n",
    "    read_csv(\"/kaggle/input/catfinal/test_labels_float32.csv\", test_labels, NUM_TEST_IMAGES, 1);\n",
    "\n",
    "    // Normalize the data\n",
    "    for (int i = 0; i < NUM_TRAIN_IMAGES * FLATTENED_SIZE; i++) {\n",
    "        train_data[i] /= 255.0;\n",
    "    }\n",
    "    for (int i = 0; i < NUM_TEST_IMAGES * FLATTENED_SIZE; i++) {\n",
    "        test_data[i] /= 255.0;\n",
    "    }\n",
    "\n",
    "    int input_size = FLATTENED_SIZE;\n",
    "    int hidden_size = 128;\n",
    "    int output_size = 1;\n",
    "\n",
    "    NeuralNetwork nn;\n",
    "    init_nn(&nn, input_size, hidden_size, output_size);\n",
    "\n",
    "    train(&nn, train_data, train_labels, 100, 0.01, BATCH_SIZE, NUM_TRAIN_IMAGES, test_data, test_labels, NUM_TEST_IMAGES);\n",
    "\n",
    "    free_nn(&nn);\n",
    "    free(train_data);\n",
    "    free(train_labels);\n",
    "    free(test_data);\n",
    "    free(test_labels);\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3d012e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T15:51:04.114857Z",
     "iopub.status.busy": "2024-06-23T15:51:04.114580Z",
     "iopub.status.idle": "2024-06-23T15:51:06.803492Z",
     "shell.execute_reply": "2024-06-23T15:51:06.802336Z"
    },
    "papermill": {
     "duration": 2.703706,
     "end_time": "2024-06-23T15:51:06.805886",
     "exception": false,
     "start_time": "2024-06-23T15:51:04.102180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvcc model_gpu.cu -o model_gpu -fmad=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a014e151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T15:51:06.831491Z",
     "iopub.status.busy": "2024-06-23T15:51:06.830899Z",
     "iopub.status.idle": "2024-06-23T15:51:40.073830Z",
     "shell.execute_reply": "2024-06-23T15:51:40.072866Z"
    },
    "papermill": {
     "duration": 33.257605,
     "end_time": "2024-06-23T15:51:40.075905",
     "exception": false,
     "start_time": "2024-06-23T15:51:06.818300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.94437852 Accuracy: 56.00%\r\n",
      "Epoch 2, Loss: 0.70524541 Accuracy: 34.00%\r\n",
      "Epoch 3, Loss: 0.65015862 Accuracy: 34.00%\r\n",
      "Epoch 4, Loss: 0.63600978 Accuracy: 34.00%\r\n",
      "Epoch 5, Loss: 0.63023589 Accuracy: 34.00%\r\n",
      "Epoch 6, Loss: 0.62374930 Accuracy: 34.00%\r\n",
      "Epoch 7, Loss: 0.61747006 Accuracy: 34.00%\r\n",
      "Epoch 8, Loss: 0.61361335 Accuracy: 34.00%\r\n",
      "Epoch 9, Loss: 0.60615193 Accuracy: 34.00%\r\n",
      "Epoch 10, Loss: 0.60142310 Accuracy: 36.00%\r\n",
      "Epoch 11, Loss: 0.59791794 Accuracy: 36.00%\r\n",
      "Epoch 12, Loss: 0.59284174 Accuracy: 36.00%\r\n",
      "Epoch 13, Loss: 0.59073313 Accuracy: 36.00%\r\n",
      "Epoch 14, Loss: 0.58852557 Accuracy: 36.00%\r\n",
      "Epoch 15, Loss: 0.58294463 Accuracy: 36.00%\r\n",
      "Epoch 16, Loss: 0.57848187 Accuracy: 40.00%\r\n",
      "Epoch 17, Loss: 0.57185822 Accuracy: 44.00%\r\n",
      "Epoch 18, Loss: 0.57231915 Accuracy: 46.00%\r\n",
      "Epoch 19, Loss: 0.56606038 Accuracy: 40.00%\r\n",
      "Epoch 20, Loss: 0.56186431 Accuracy: 46.00%\r\n",
      "Epoch 21, Loss: 0.55685367 Accuracy: 50.00%\r\n",
      "Epoch 22, Loss: 0.55751762 Accuracy: 50.00%\r\n",
      "Epoch 23, Loss: 0.55476107 Accuracy: 44.00%\r\n",
      "Epoch 24, Loss: 0.54472264 Accuracy: 54.00%\r\n",
      "Epoch 25, Loss: 0.54477724 Accuracy: 54.00%\r\n",
      "Epoch 26, Loss: 0.54354569 Accuracy: 50.00%\r\n",
      "Epoch 27, Loss: 0.53777443 Accuracy: 54.00%\r\n",
      "Epoch 28, Loss: 0.53273667 Accuracy: 50.00%\r\n",
      "Epoch 29, Loss: 0.52259798 Accuracy: 54.00%\r\n",
      "Epoch 30, Loss: 0.52718059 Accuracy: 54.00%\r\n",
      "Epoch 31, Loss: 0.52284048 Accuracy: 58.00%\r\n",
      "Epoch 32, Loss: 0.52261418 Accuracy: 54.00%\r\n",
      "Epoch 33, Loss: 0.51260888 Accuracy: 56.00%\r\n",
      "Epoch 34, Loss: 0.51622503 Accuracy: 56.00%\r\n",
      "Epoch 35, Loss: 0.50549322 Accuracy: 58.00%\r\n",
      "Epoch 36, Loss: 0.50838553 Accuracy: 60.00%\r\n",
      "Epoch 37, Loss: 0.49992691 Accuracy: 64.00%\r\n",
      "Epoch 38, Loss: 0.50408956 Accuracy: 58.00%\r\n",
      "Epoch 39, Loss: 0.49598605 Accuracy: 64.00%\r\n",
      "Epoch 40, Loss: 0.49685836 Accuracy: 60.00%\r\n",
      "Epoch 41, Loss: 0.48545657 Accuracy: 60.00%\r\n",
      "Epoch 42, Loss: 0.48705711 Accuracy: 64.00%\r\n",
      "Epoch 43, Loss: 0.48709698 Accuracy: 68.00%\r\n",
      "Epoch 44, Loss: 0.48558701 Accuracy: 64.00%\r\n",
      "Epoch 45, Loss: 0.48506709 Accuracy: 66.00%\r\n",
      "Epoch 46, Loss: 0.47500827 Accuracy: 70.00%\r\n",
      "Epoch 47, Loss: 0.47716211 Accuracy: 64.00%\r\n",
      "Epoch 48, Loss: 0.46660424 Accuracy: 62.00%\r\n",
      "Epoch 49, Loss: 0.45778110 Accuracy: 70.00%\r\n",
      "Epoch 50, Loss: 0.46555514 Accuracy: 72.00%\r\n",
      "Epoch 51, Loss: 0.46766982 Accuracy: 72.00%\r\n",
      "Epoch 52, Loss: 0.46948220 Accuracy: 74.00%\r\n",
      "Epoch 53, Loss: 0.46869165 Accuracy: 76.00%\r\n",
      "Epoch 54, Loss: 0.47067100 Accuracy: 76.00%\r\n",
      "Epoch 55, Loss: 0.47211473 Accuracy: 70.00%\r\n",
      "Epoch 56, Loss: 0.44027270 Accuracy: 64.00%\r\n",
      "Epoch 57, Loss: 0.43479839 Accuracy: 68.00%\r\n",
      "Epoch 58, Loss: 0.43305710 Accuracy: 74.00%\r\n",
      "Epoch 59, Loss: 0.43477178 Accuracy: 72.00%\r\n",
      "Epoch 60, Loss: 0.43563706 Accuracy: 76.00%\r\n",
      "Epoch 61, Loss: 0.44057064 Accuracy: 76.00%\r\n",
      "Epoch 62, Loss: 0.44373839 Accuracy: 76.00%\r\n",
      "Epoch 63, Loss: 0.44644546 Accuracy: 78.00%\r\n",
      "Epoch 64, Loss: 0.43919821 Accuracy: 76.00%\r\n",
      "Epoch 65, Loss: 0.43093125 Accuracy: 76.00%\r\n",
      "Epoch 66, Loss: 0.42180950 Accuracy: 76.00%\r\n",
      "Epoch 67, Loss: 0.42690676 Accuracy: 78.00%\r\n",
      "Epoch 68, Loss: 0.42397586 Accuracy: 76.00%\r\n",
      "Epoch 69, Loss: 0.41856804 Accuracy: 78.00%\r\n",
      "Epoch 70, Loss: 0.40309069 Accuracy: 76.00%\r\n",
      "Epoch 71, Loss: 0.40113854 Accuracy: 78.00%\r\n",
      "Epoch 72, Loss: 0.40312047 Accuracy: 78.00%\r\n",
      "Epoch 73, Loss: 0.40150610 Accuracy: 76.00%\r\n",
      "Epoch 74, Loss: 0.40419291 Accuracy: 80.00%\r\n",
      "Epoch 75, Loss: 0.40609242 Accuracy: 76.00%\r\n",
      "Epoch 76, Loss: 0.41083649 Accuracy: 76.00%\r\n",
      "Epoch 77, Loss: 0.41442337 Accuracy: 78.00%\r\n",
      "Epoch 78, Loss: 0.39671931 Accuracy: 78.00%\r\n",
      "Epoch 79, Loss: 0.38597883 Accuracy: 78.00%\r\n",
      "Epoch 80, Loss: 0.37904292 Accuracy: 78.00%\r\n",
      "Epoch 81, Loss: 0.37731357 Accuracy: 80.00%\r\n",
      "Epoch 82, Loss: 0.37192994 Accuracy: 78.00%\r\n",
      "Epoch 83, Loss: 0.36568066 Accuracy: 78.00%\r\n",
      "Epoch 84, Loss: 0.36130785 Accuracy: 78.00%\r\n",
      "Epoch 85, Loss: 0.35995879 Accuracy: 78.00%\r\n",
      "Epoch 86, Loss: 0.36088162 Accuracy: 80.00%\r\n",
      "Epoch 87, Loss: 0.37004559 Accuracy: 76.00%\r\n",
      "Epoch 88, Loss: 0.39321964 Accuracy: 78.00%\r\n",
      "Epoch 89, Loss: 0.39919211 Accuracy: 80.00%\r\n",
      "Epoch 90, Loss: 0.35591615 Accuracy: 78.00%\r\n",
      "Epoch 91, Loss: 0.34851778 Accuracy: 80.00%\r\n",
      "Epoch 92, Loss: 0.34364304 Accuracy: 78.00%\r\n",
      "Epoch 93, Loss: 0.34262565 Accuracy: 80.00%\r\n",
      "Epoch 94, Loss: 0.34228946 Accuracy: 80.00%\r\n",
      "Epoch 95, Loss: 0.34307715 Accuracy: 78.00%\r\n",
      "Epoch 96, Loss: 0.35388356 Accuracy: 78.00%\r\n",
      "Epoch 97, Loss: 0.37762504 Accuracy: 78.00%\r\n",
      "Epoch 98, Loss: 0.38662101 Accuracy: 78.00%\r\n",
      "Epoch 99, Loss: 0.35409701 Accuracy: 80.00%\r\n",
      "Epoch 100, Loss: 0.34421810 Accuracy: 78.00%\r\n"
     ]
    }
   ],
   "source": [
    "!./model_gpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e869ac",
   "metadata": {
    "papermill": {
     "duration": 0.018548,
     "end_time": "2024-06-23T15:51:40.113789",
     "exception": false,
     "start_time": "2024-06-23T15:51:40.095241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5191414,
     "sourceId": 8663973,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1146.507524,
   "end_time": "2024-06-23T15:51:40.352557",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-23T15:32:33.845033",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
